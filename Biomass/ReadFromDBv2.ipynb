{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is intended to show how to use pandas, and sql alchemy to upload data into DB2-switch and create geospatial coordinate and indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install using pip or any other package manager pandas, sqlalchemy and pg8000. The later one is the driver to connect to the db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required packages, first create the engine to connect to the DB. The approach I generally use is to create a string based on the username and password. The code is a function, you just need to fill in with the username, password and the dbname. \n",
    "\n",
    "It allows you to create different engines to connect to serveral dbs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def connection(user,passwd,dbname, echo_i=False):\n",
    "    str1 = ('postgresql+pg8000://' + user +':' + passw + '@switch-db2.erg.berkeley.edu:5432/' \n",
    "            + dbname + '?ssl=true&sslfactory=org.postgresql.ssl.NonValidatingFactory')\n",
    "    engine = create_engine(str1,echo=echo_i,isolation_level='AUTOCOMMIT')\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = 'jdlara'\n",
    "passw = 'Amadeus-2010'\n",
    "dbname = 'apl_cec' \n",
    "engine_db= connection(user,passw,dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, use pandas to import the data from Excel files or any other text file format. Make sure that the data in good shape before trying to push it into the server. In this example I use previous knowledge of the structure of the tabs in the excel file to recursively upload each tab and match the name of the table with the tab name. \n",
    "\n",
    "If you are using csv files just change the commands to ```pd.read_csv()``` in this [link](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) you can find the documentation. \n",
    "\n",
    "**Before doing this I already checked that the data is properly organized, crate new cells to explore the data beforehand if needed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "excel_file = 'substations_table.xlsx'\n",
    "tab_name = 'sheet1'\n",
    "schema_for_upload = 'geographic_data'\n",
    "pd_data.to_sql(name, engine_db, schema=schema_for_upload, if_exists='replace',chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#excel_file = 'substations_table.xlsx'\n",
    "#tab_name = 'sheet1'\n",
    "csv_name = ['LEMMA_ADS_AllSpp_2016_Turbo_01252016.csv']\n",
    "schema_for_upload = 'lemma2016'\n",
    "for name in csv_name:\n",
    "    pd_data = pd.read_csv(name, encoding='UTF-8')\n",
    "    pd_data.to_sql(name, engine_db, schema=schema_for_upload, if_exists='replace',chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once the data is updated, it is possible to run the SQL commands to properly create ```geom``` columns in the tables, this can be done as follows. The ojective is to run an SQL querie like this: \n",
    "\n",
    "```PGSQL\n",
    "set search_path = SCHEMA, public;\n",
    "alter table vTABLE drop column if exists geom;\n",
    "SELECT AddGeometryColumn ('SCHEMA','vTABLE','geom',4326,'POINT',2);\n",
    "UPDATE TABLE set geom = ST_SetSRID(st_makepoint(vTABLE.lon, vTABLE.lat), 4326)::geometry;\n",
    "```\n",
    "where ```SCHEMA``` and ```vTABLE``` are the variable portions. Also note, that this query assumes that your columns with latitude and longitude are named ```lat``` and ```lon``` respectively; moreover, it also assumes that the coordinates are in the 4326 [projection](https://www.gislounge.com/map-projection/). \n",
    "\n",
    "The following function runs the query for you, considering again that the data is clean and nice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_geom(table,schema,engine, projection=5070):\n",
    "    k = engine.connect()\n",
    "    query = ('set search_path = \"'+ schema +'\"'+ ', public;')\n",
    "    print query\n",
    "    k.execute(query)\n",
    "    query = ('alter table ' + table + ' drop column if exists geom;')\n",
    "    print query\n",
    "    k.execute(query)\n",
    "    query = 'SELECT AddGeometryColumn (\\''+ schema + '\\',\\''+ table + '\\',\\'geom\\''+',5070,\\'POINT\\',2);'\n",
    "    print query\n",
    "    k.execute(query)\n",
    "    query = ('UPDATE ' + table + ' set geom = ST_SetSRID(st_makepoint(' + table + '.x, ' + \n",
    "             table + '.y),' + str(projection) + ')::geometry;')\n",
    "    k.execute(query)\n",
    "    print query\n",
    "    k = engine.dispose()\n",
    "    return 'geom column added with SRID ' + str(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set search_path = \"lemma2016\", public;\n",
      "alter table results_approach1 drop column if exists geom;\n",
      "SELECT AddGeometryColumn ('lemma2016','results_approach1','geom',5070,'POINT',2);\n",
      "UPDATE results_approach1 set geom = ST_SetSRID(st_makepoint(results_approach1.x, results_approach1.y),5070)::geometry;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'geom column added with SRID 5070'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = 'results_approach1'\n",
    "schema = 'lemma2016'\n",
    "create_geom(table,schema,engine_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function created the geom column, the next step is to define a function to create the Primary-Key in the db. Remember that the index from the data frame is included as an index in the db, sometimes an index is not really neded and might need to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_pk(table,schema,column,engine):\n",
    "    k = engine.connect()\n",
    "    query = ('set search_path = \"'+ schema +'\"'+ ', public;')\n",
    "    print query\n",
    "    k.execute(query)\n",
    "    query = ('alter table ' + table + ' ADD CONSTRAINT '+ table +'_pk PRIMARY KEY (' + column + ')')\n",
    "    print query \n",
    "    k.execute(query)\n",
    "    k = engine.dispose()\n",
    "    return 'Primary key created with column' + column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = ''\n",
    "create_pk(table,schema,col,engine_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we use postgis is to improve geospatial queries and provide a better data structure for geospatial operations. Many of the ```ST_``` functions have improved performance when a geospatial index is created. The process implemented here comes from this [workshop](http://revenant.ca/www/postgis/workshop/indexing.html). This re-creates the process using python functions so that it can be easily replicated for many tables.\n",
    "\n",
    "The query to create a geospatial index is as follows: \n",
    "\n",
    "```SQL\n",
    "set search_path = SCHEMA, public;\n",
    "CREATE INDEX vTABLE_gix ON vTABLE USING GIST (geom);\n",
    "```\n",
    "This assumes that the column name with the geometry is named ```geom```. If the process follows from the previous code, it will work ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is to run a ```VACUUM```, creating an index is not enough to allow PostgreSQL to use it effectively. VACUUMing must be performed when ever a new index is created or after a large number of UPDATEs, INSERTs or DELETEs are issued against a table. \n",
    "\n",
    "```SQL\n",
    "VACUUM ANALYZE vTABLE;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step corresponds to ```CLUSTERING```, this process re-orders the table according to the geospatial index we created. This ensures that records with similar attributes have a high likelihood of being found in the same page, reducing the number of pages that must be read into memory for some types of queries. When a query to find nearest neighbors or within a certain are is needed, geometries that are near each other in space are near each other on disk. The query to perform this clustering is as follows:\n",
    "\n",
    "```\n",
    "CLUSTER vTABLE USING vTABLE_gix;\n",
    "ANALYZE vTABLE;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gidx(table,schema,engine,column='geom'):\n",
    "    k = engine.connect()\n",
    "    query = ('set search_path = \"'+ schema +'\"'+ ', public;')\n",
    "    k.execute(query)\n",
    "    print query\n",
    "    query = ('CREATE INDEX ' + table + '_gix ON ' + table + ' USING GIST (' + column + ');')\n",
    "    k.execute(query)\n",
    "    print query\n",
    "    query = ('VACUUM ' + table + ';')\n",
    "    k.execute(query)\n",
    "    print query\n",
    "    query = ('CLUSTER ' + table + ' USING ' + table + '_gix;')\n",
    "    k.execute(query)\n",
    "    print query\n",
    "    query = ('ANALYZE ' + table + ';')\n",
    "    k.execute(query)\n",
    "    print query\n",
    "    k = engine.dispose()\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set search_path = \"geographic_data\", public;\n",
      "CREATE INDEX substation_table_gix ON substation_table USING GIST (geom);\n",
      "VACUUM substation_table;\n",
      "CLUSTER substation_table USING substation_table_gix;\n",
      "ANALYZE substation_table;\n"
     ]
    }
   ],
   "source": [
    "create_gidx(table,schema,engine_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
